{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1. \n",
    "### A company conducted a survey of its employees and found that 70% of the employees use the company's health insurance plan, while 40% of the employees who use the plan are smokers. What is the probability that an employee is a smoker given that he/she uses the health insurance plan?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The probability that an employee is a smoker given that he/she uses the health insurance plan can be calculated using the conditional probability formula:\n",
    "\n",
    "\\[ P(\\text{Smoker} | \\text{Uses Health Insurance}) = \\frac{P(\\text{Smoker and Uses Health Insurance})}{P(\\text{Uses Health Insurance})} \\]\n",
    "\n",
    "From the information provided:\n",
    "- The probability that an employee uses the health insurance plan is 70%, denoted as \\( P(\\text{Uses Health Insurance}) = 0.70 \\).\n",
    "- The probability that an employee uses the health insurance plan and is a smoker is 40%, denoted as \\( P(\\text{Smoker and Uses Health Insurance}) = 0.40 \\).\n",
    "\n",
    "Now, plug these values into the formula:\n",
    "\n",
    "\\[ P(\\text{Smoker} | \\text{Uses Health Insurance}) = \\frac{0.40}{0.70} \\]\n",
    "\n",
    "Calculate the result:\n",
    "\n",
    "\\[ P(\\text{Smoker} | \\text{Uses Health Insurance}) \\approx \\frac{4}{7} \\]\n",
    "\n",
    "Therefore, the probability that an employee is a smoker given that he/she uses the health insurance plan is approximately \\( \\frac{4}{7} \\)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2. \n",
    "### What is the difference between Bernoulli Naive Bayes and Multinomial Naive Bayes?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The key difference between Bernoulli Naive Bayes and Multinomial Naive Bayes lies in the type of data they are designed to handle and the assumptions they make about the distribution of the features.\n",
    "\n",
    "**1. Data Type:**\n",
    "\n",
    "- **Bernoulli Naive Bayes:**\n",
    "  - Designed for binary or Boolean data.\n",
    "  - Each feature is treated as a binary variable, indicating the presence or absence of a particular attribute.\n",
    "  - Commonly used for document classification tasks where the focus is on whether certain words are present in a document (bag-of-words representation).\n",
    "  \n",
    "- **Multinomial Naive Bayes:**\n",
    "  - Designed for discrete data, often used for text classification tasks.\n",
    "  - Suitable for problems where features represent counts or frequencies (e.g., word counts in a document).\n",
    "\n",
    "**2. Feature Representation:**\n",
    "\n",
    "- **Bernoulli Naive Bayes:**\n",
    "  - Features are represented as binary values (0 or 1) indicating the absence or presence of a particular attribute.\n",
    "  - Assumes that the absence of a feature is as informative as its presence.\n",
    "\n",
    "- **Multinomial Naive Bayes:**\n",
    "  - Features are represented as counts or frequencies.\n",
    "  - Well-suited for problems where the order of occurrences or the frequency of features is important, such as in natural language processing.\n",
    "\n",
    "**3. Independence Assumption:**\n",
    "\n",
    "- Both Bernoulli and Multinomial Naive Bayes assume feature independence given the class label. However, they differ in how they model and handle feature values.\n",
    "\n",
    "In summary, the choice between Bernoulli Naive Bayes and Multinomial Naive Bayes depends on the nature of the data and the specific requirements of the classification problem. If the features are binary and the focus is on presence/absence, Bernoulli Naive Bayes may be more appropriate. If the features are counts or frequencies and the order of occurrences matters, Multinomial Naive Bayes is a better choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3. \n",
    "### How does Bernoulli Naive Bayes handle missing values?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bernoulli Naive Bayes handles missing values by treating them as a specific category or by incorporating the missing values into the feature representation. The approach depends on the implementation or the specific requirements of the problem. Here are two common ways to handle missing values in Bernoulli Naive Bayes:\n",
    "\n",
    "1. **Treating Missing Values as a Separate Category:**\n",
    "   - One approach is to consider missing values as a distinct category, treating them as a separate state of the binary feature. This means that for each feature with a missing value, a separate category is created, and the presence or absence of the feature is not considered. The model then estimates the probability of each class given the presence, absence, or missing status of the feature.\n",
    "\n",
    "2. **Incorporating Missing Values as a Feature State:**\n",
    "   - Another approach is to incorporate the missing values into the feature representation. Instead of treating missing values as a separate category, the missing status is considered as an additional state of the binary feature. In this case, the model estimates the probability of each class given the presence or absence (including missing) of the feature.\n",
    "\n",
    "The choice between these approaches depends on the nature of the data and the specific requirements of the classification problem. It's essential to consider the impact of missing values on the model's performance and the assumptions made about the missing data.\n",
    "\n",
    "In practice, the handling of missing values may also depend on the implementation of the Bernoulli Naive Bayes algorithm in a particular library or software package. Some implementations may provide options to handle missing values in different ways, allowing users to choose the approach that best fits their needs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q4. \n",
    "### Can Gaussian Naive Bayes be used for multi-class classification?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, Gaussian Naive Bayes can be used for multi-class classification. Gaussian Naive Bayes is an extension of the Naive Bayes algorithm that assumes continuous-valued features follow a Gaussian (normal) distribution within each class. While the original Naive Bayes algorithm is often used for binary and multi-class classification problems, Gaussian Naive Bayes specifically handles continuous data.\n",
    "\n",
    "In the context of multi-class classification, Gaussian Naive Bayes can be extended straightforwardly. The idea is to estimate the parameters (mean and variance) of the Gaussian distribution for each feature within each class. When making predictions for a new instance, the algorithm calculates the likelihood of the observed feature values under each class's Gaussian distribution and combines this information with prior probabilities to determine the most likely class.\n",
    "\n",
    "The general formula for Gaussian Naive Bayes in the context of multi-class classification is as follows:\n",
    "\n",
    "\\[ P(C_k | x) = \\frac{P(x | C_k) \\cdot P(C_k)}{P(x)} \\]\n",
    "\n",
    "Here:\n",
    "- \\( P(C_k | x) \\) is the posterior probability of class \\(C_k\\) given the features \\(x\\).\n",
    "- \\( P(x | C_k) \\) is the likelihood of the features \\(x\\) given class \\(C_k\\), and it is modeled using the Gaussian distribution parameters.\n",
    "- \\( P(C_k) \\) is the prior probability of class \\(C_k\\).\n",
    "- \\( P(x) \\) is the marginal probability of the features \\(x\\), and it serves as a normalizing constant.\n",
    "\n",
    "In summary, Gaussian Naive Bayes can be applied to multi-class classification problems by extending the algorithm to estimate Gaussian distribution parameters for each feature within each class and using these parameters to make predictions for new instances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q5. Assignment:\n",
    "\n",
    "### Data preparation:\n",
    "\n",
    "Download the \"Spambase Data Set\" from the UCI Machine Learning Repository (https://archive.ics.uci.edu/ml/\n",
    "datasets/Spambase). This dataset contains email messages, where the goal is to predict whether a message\n",
    "is spam or not based on several input features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementation:\n",
    "\n",
    "Implement Bernoulli Naive Bayes, Multinomial Naive Bayes, and Gaussian Naive Bayes classifiers using the\n",
    "scikit-learn library in Python. Use 10-fold cross-validation to evaluate the performance of each classifier on the\n",
    "dataset. You should use the default hyperparameters for each classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results:\n",
    "\n",
    "Report the following performance metrics for each classifier:\n",
    "\n",
    "Accuracy\n",
    "Precision\n",
    "Recall\n",
    "F1 score\n",
    "\n",
    "**Discussion:**\n",
    "\n",
    "\n",
    "Discuss the results you obtained. Which variant of Naive Bayes performed the best? Why do you think that is\n",
    "the case? Are there any limitations of Naive Bayes that you observed?\n",
    "\n",
    "\n",
    "**Conclusion:**\n",
    "\n",
    "Summarise your findings and provide some suggestions for future work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB, GaussianNB\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Load the dataset\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.data\"\n",
    "column_names = [...]  # Provide the column names based on the dataset description\n",
    "data = pd.read_csv(url, header=None, names=column_names)\n",
    "\n",
    "# Split the data into features (X) and target variable (y)\n",
    "X = data.drop('target_column', axis=1)  # Adjust 'target_column' based on your dataset\n",
    "y = data['target_column']\n",
    "\n",
    "# Initialize classifiers\n",
    "bernoulli_nb = BernoulliNB()\n",
    "multinomial_nb = MultinomialNB()\n",
    "gaussian_nb = GaussianNB()\n",
    "\n",
    "classifiers = [bernoulli_nb, multinomial_nb, gaussian_nb]\n",
    "classifiers_names = ['Bernoulli Naive Bayes', 'Multinomial Naive Bayes', 'Gaussian Naive Bayes']\n",
    "\n",
    "# Evaluation metrics\n",
    "metrics = ['accuracy', 'precision', 'recall', 'f1']\n",
    "\n",
    "# Perform 10-fold cross-validation for each classifier\n",
    "for classifier, name in zip(classifiers, classifiers_names):\n",
    "    kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "    for metric in metrics:\n",
    "        scores = cross_val_score(classifier, X, y, cv=kf, scoring=metric)\n",
    "        print(f'{name} - {metric}: {scores.mean()}')\n",
    "\n",
    "# Discuss the results, limitations, and conclusion\n",
    "# ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Completed_10th_April_Assignment:\n",
    "## __________________________"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
