{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1.\n",
    "## What is the relationship between polynomial functions and kernel functions in machine learning algorithms?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In machine learning algorithms, kernel functions play a crucial role, especially in Support Vector Machines (SVMs). The relationship between polynomial functions and kernel functions lies in the use of polynomial kernels as a type of kernel function in various machine learning algorithms, with SVMs being a notable example.\n",
    "\n",
    "### Polynomial Kernels:\n",
    "\n",
    "A polynomial kernel is a type of kernel function that is used to implicitly map input data into a higher-dimensional space. The general form of a polynomial kernel is:\n",
    "\n",
    "\\[ K(\\mathbf{x}_i, \\mathbf{x}_j) = (\\langle \\mathbf{x}_i, \\mathbf{x}_j \\rangle + c)^d \\]\n",
    "\n",
    "where:\n",
    "- \\( \\langle \\mathbf{x}_i, \\mathbf{x}_j \\rangle \\) is the dot product of the input vectors \\( \\mathbf{x}_i \\) and \\( \\mathbf{x}_j \\).\n",
    "- \\( c \\) is a constant.\n",
    "- \\( d \\) is the degree of the polynomial.\n",
    "\n",
    "### Relationship:\n",
    "\n",
    "1. **Mapping to Higher Dimension:**\n",
    "   - Polynomial kernels provide a way to implicitly map data into a higher-dimensional space without explicitly computing the transformation. The term \\( (\\langle \\mathbf{x}_i, \\mathbf{x}_j \\rangle + c)^d \\) captures the inner product of the original data points in this higher-dimensional space.\n",
    "\n",
    "2. **Generalization of Polynomial Functions:**\n",
    "   - Polynomial kernels generalize the concept of polynomial functions to higher-dimensional feature spaces. In traditional polynomial regression or classification, one explicitly performs the polynomial transformation on input features. In contrast, polynomial kernels achieve a similar effect without the need to compute the transformed features explicitly.\n",
    "\n",
    "3. **SVMs and Kernel Trick:**\n",
    "   - Support Vector Machines use the kernel trick, which involves replacing the dot product \\( \\langle \\mathbf{x}_i, \\mathbf{x}_j \\rangle \\) with a kernel function \\( K(\\mathbf{x}_i, \\mathbf{x}_j) \\). The polynomial kernel is one of the choices for the kernel function in SVMs, allowing SVMs to model non-linear decision boundaries in the input space.\n",
    "\n",
    "4. **Flexibility in Capturing Non-Linearity:**\n",
    "   - Polynomial kernels, with their adjustable degree parameter \\( d \\), provide a flexible way to capture non-linear relationships in the data. Higher-degree polynomials can capture more complex patterns in the data.\n",
    "\n",
    "### Example:\n",
    "\n",
    "Consider a simple case where the input data is one-dimensional (single feature), and we want to use a polynomial kernel in an SVM to capture a quadratic relationship. The polynomial kernel would look like:\n",
    "\n",
    "\\[ K(x_i, x_j) = (x_i \\cdot x_j + c)^2 \\]\n",
    "\n",
    "This allows the SVM to implicitly map the input data into a higher-dimensional space where a linear decision boundary might be sufficient.\n",
    "\n",
    "In summary, polynomial kernels in machine learning provide a way to generalize polynomial functions to higher-dimensional spaces, and they play a crucial role in algorithms like SVMs for capturing non-linear patterns in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2. \n",
    "## How can we implement an SVM with a polynomial kernel in Python using Scikit-learn?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load Iris dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Consider only the first two classes (0 and 1) for binary classification\n",
    "X = X[y != 2]\n",
    "y = y[y != 2]\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train_std = scaler.fit_transform(X_train)\n",
    "X_test_std = scaler.transform(X_test)\n",
    "\n",
    "# Create an SVM model with a polynomial kernel\n",
    "svm_poly = SVC(kernel='poly', degree=3, C=1.0)  # Polynomial kernel of degree 3\n",
    "\n",
    "# Train the SVM model\n",
    "svm_poly.fit(X_train_std, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_poly = svm_poly.predict(X_test_std)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy_poly = accuracy_score(y_test, y_pred_poly)\n",
    "print(\"Accuracy (Polynomial Kernel):\", accuracy_poly)\n",
    "\n",
    "# Visualize decision boundary\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "h = 0.02\n",
    "x_min, x_max = X_train_std[:, 0].min() - 1, X_train_std[:, 0].max() + 1\n",
    "y_min, y_max = X_train_std[:, 1].min() - 1, X_train_std[:, 1].max() + 1\n",
    "\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "Z = svm_poly.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.contourf(xx, yy, Z, cmap=plt.cm.Paired, alpha=0.8)\n",
    "\n",
    "# Plot the training points\n",
    "plt.scatter(X_train_std[:, 0], X_train_std[:, 1], c=y_train, cmap=plt.cm.Paired, edgecolors='k')\n",
    "# Plot the testing points\n",
    "plt.scatter(X_test_std[:, 0], X_test_std[:, 1], c=y_test, cmap=plt.cm.Paired, marker='x', s=100, edgecolors='k')\n",
    "\n",
    "plt.title('SVM Decision Boundary with Polynomial Kernel')\n",
    "plt.xlabel('Sepal Length (standardized)')\n",
    "plt.ylabel('Sepal Width (standardized)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3. \n",
    "## How does increasing the value of epsilon affect the number of support vectors in SVR?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Support Vector Regression (SVR), epsilon (\\(\\varepsilon\\)) is a parameter that defines the width of the epsilon-insensitive tube around the regression line. This tube determines the tolerance for errors in the training data, allowing some points to fall outside the tube without affecting the model's performance. The points inside the tube or on its boundary are considered support vectors.\n",
    "\n",
    "The epsilon-insensitive loss function for SVR is defined as follows:\n",
    "\n",
    "\\[ L(\\epsilon, y, f(\\mathbf{x})) = \\max(0, |y - f(\\mathbf{x})| - \\varepsilon) \\]\n",
    "\n",
    "Here:\n",
    "- \\( y \\) is the true target value.\n",
    "- \\( f(\\mathbf{x}) \\) is the predicted value.\n",
    "- \\( \\varepsilon \\) is the width of the epsilon-insensitive tube.\n",
    "\n",
    "Now, let's explore how increasing the value of epsilon affects the number of support vectors:\n",
    "\n",
    "1. **Smaller Epsilon (\\(\\varepsilon\\)):**\n",
    "   - A smaller epsilon results in a narrower epsilon-insensitive tube.\n",
    "   - The model becomes more sensitive to errors, requiring predictions to be closer to the true targets.\n",
    "   - This may lead to a larger number of support vectors, as the model is less tolerant of deviations from the true targets.\n",
    "\n",
    "2. **Larger Epsilon (\\(\\varepsilon\\)):**\n",
    "   - A larger epsilon allows for a wider epsilon-insensitive tube.\n",
    "   - The model becomes more tolerant of errors, allowing predictions to deviate by a larger margin from the true targets.\n",
    "   - This may result in a smaller number of support vectors, as the model is more lenient in terms of accommodating errors within the wider tube.\n",
    "\n",
    "In summary, increasing the value of epsilon generally tends to decrease the number of support vectors in SVR, as it allows for a wider margin of tolerance for errors. The choice of epsilon depends on the specific characteristics of the data and the desired balance between model flexibility and robustness to noise. It is often determined through cross-validation or grid search to find an optimal value for the given problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q4. \n",
    "## How does the choice of kernel function, C parameter, epsilon parameter, and gamma parameter affect the performance of Support Vector Regression (SVR)? Can you explain how each parameter works and provide examples of when you might want to increase or decrease its value?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Support Vector Regression (SVR) is a powerful algorithm for regression tasks in machine learning. The performance of SVR is influenced by several parameters, including the choice of kernel function, the C parameter, the epsilon parameter (\\(\\varepsilon\\)), and the gamma parameter (\\(\\gamma\\)). Let's discuss each parameter and its impact on SVR:\n",
    "\n",
    "1. **Kernel Function:**\n",
    "   - **Explanation:** The kernel function determines the type of transformation applied to the input data. Common choices include linear, polynomial, radial basis function (RBF), and sigmoid kernels.\n",
    "   - **Impact:**\n",
    "      - A linear kernel assumes a linear relationship between features and may work well for linearly separable data.\n",
    "      - Polynomial kernels allow SVR to capture non-linear relationships by using higher-degree polynomials.\n",
    "      - RBF kernels are versatile and can capture complex non-linear patterns, but they are sensitive to the gamma parameter.\n",
    "      - The choice depends on the nature of the data and the complexity of the underlying relationships.\n",
    "   - **Example:**\n",
    "      - Use a linear kernel for data with a clear linear relationship.\n",
    "      - Use an RBF kernel for data with complex, non-linear patterns.\n",
    "\n",
    "2. **C Parameter:**\n",
    "   - **Explanation:** The C parameter controls the trade-off between achieving a low training error and a smooth decision boundary. A smaller C allows for a softer margin with more support vectors, while a larger C enforces a harder margin, potentially leading to fewer support vectors.\n",
    "   - **Impact:**\n",
    "      - Smaller C values lead to a more flexible model that allows errors in the training data.\n",
    "      - Larger C values result in a more rigid model that aims to minimize errors.\n",
    "   - **Example:**\n",
    "      - Use a smaller C if the training data has noise or outliers.\n",
    "      - Use a larger C if you want to penalize errors more heavily, especially when the data is less noisy.\n",
    "\n",
    "3. **Epsilon Parameter (\\(\\varepsilon\\)):**\n",
    "   - **Explanation:** The epsilon parameter defines the width of the epsilon-insensitive tube around the regression line. It determines the tolerance for errors in the training data.\n",
    "   - **Impact:**\n",
    "      - Smaller values make the model less tolerant of errors, potentially leading to a smaller tube and more support vectors.\n",
    "      - Larger values allow for a wider tube, making the model more tolerant of errors and potentially reducing the number of support vectors.\n",
    "   - **Example:**\n",
    "      - Use a smaller \\(\\varepsilon\\) if you want the model to be sensitive to small errors in the training data.\n",
    "      - Use a larger \\(\\varepsilon\\) if you want the model to be more robust to noise or minor variations in the data.\n",
    "\n",
    "4. **Gamma Parameter (\\(\\gamma\\)):**\n",
    "   - **Explanation:** The gamma parameter is specific to RBF kernels and determines the shape of the decision boundary. A smaller gamma results in a broader decision boundary, while a larger gamma makes the decision boundary more localized.\n",
    "   - **Impact:**\n",
    "      - Smaller gamma values lead to smoother decision boundaries, making the model less prone to overfitting.\n",
    "      - Larger gamma values result in more complex and localized decision boundaries, potentially leading to overfitting.\n",
    "   - **Example:**\n",
    "      - Use a smaller \\(\\gamma\\) for a broader decision boundary when dealing with larger datasets or when the underlying pattern is smooth.\n",
    "      - Use a larger \\(\\gamma\\) for more localized decision boundaries when the data is complex and requires a more detailed model.\n",
    "\n",
    "In practice, the optimal values for these parameters are often found through hyperparameter tuning techniques such as grid search or randomized search, coupled with cross-validation. It's essential to consider the characteristics of the data and the specific requirements of the problem when selecting parameter values. Adjusting these parameters can significantly impact the performance and generalization ability of an SVR model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q5. Assignment:\n",
    "- Import the necessary libraries and load the dataset.\n",
    "\n",
    "- Split the dataset into training and testing set.\n",
    "\n",
    "- Preprocess the data using any technique of your choice (e.g. scaling, normaliMation.)\n",
    "\n",
    "- Create an instance of the SVC classifier and train it on the training data.\n",
    "\n",
    "- Use the trained classifier to predict the labels of the testing data.\n",
    "\n",
    "- Evaluate the performance of the classifier using any metric of your choice (e.g. accuracy,precision, recall, F1-score)\n",
    "\n",
    "- Tune the hyperparameters of the SVC classifier using GridSearchCV or RandomiMedSearchCV to improve its performance\n",
    "\n",
    "- Train the tuned classifier on the entire dataset.\n",
    "\n",
    "- Save the trained classifier to a file for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import joblib  # To save the trained model\n",
    "\n",
    "# Load the dataset (replace X and y with your actual dataset)\n",
    "# Example assuming you have a dataset named 'data.csv'\n",
    "# import pandas as pd\n",
    "# data = pd.read_csv('data.csv')\n",
    "# X = data.drop('label', axis=1)  # Assuming 'label' is the target variable\n",
    "# y = data['label']\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Preprocess the data (scaling in this case)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Create an instance of the SVC classifier\n",
    "svm_classifier = SVC()\n",
    "\n",
    "# Train the classifier on the training data\n",
    "svm_classifier.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Use the trained classifier to predict the labels of the testing data\n",
    "y_pred = svm_classifier.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the performance of the classifier\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "classification_rep = classification_report(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Classification Report:\\n\", classification_rep)\n",
    "\n",
    "# Tune hyperparameters using GridSearchCV\n",
    "param_grid = {'C': [0.1, 1, 10], 'gamma': [0.01, 0.1, 1], 'kernel': ['linear', 'rbf']}\n",
    "grid_search = GridSearchCV(SVC(), param_grid, cv=3)\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Get the best parameters\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "\n",
    "# Train the tuned classifier on the entire dataset\n",
    "tuned_svm_classifier = grid_search.best_estimator_\n",
    "tuned_svm_classifier.fit(X_scaled, y)  # Assuming X_scaled is the entire dataset\n",
    "\n",
    "# Save the trained classifier to a file for future use\n",
    "joblib.dump(tuned_svm_classifier, 'tuned_svm_classifier.pkl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Completed_7th_April_Assignment:\n",
    "## ______________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
