{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1. \n",
    "### What is Gradient Boosting Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Boosting Regression is a machine learning technique that belongs to the family of ensemble learning methods. It is particularly effective for regression tasks, where the goal is to predict a continuous numeric outcome. Gradient Boosting Regression builds a strong predictive model by combining the predictions of multiple weak learners, typically decision trees.\n",
    "\n",
    "Here's an overview of how Gradient Boosting Regression works:\n",
    "\n",
    "### 1. **Initialization:**\n",
    "   - The initial prediction is set to the average of the target variable (the mean for regression tasks).\n",
    "\n",
    "### 2. **Building Weak Learners (Decision Trees):**\n",
    "   - A weak learner (usually a decision tree) is trained to predict the residuals (differences between the actual and predicted values) from the current model.\n",
    "   - The weak learner is fit to the residuals, emphasizing the instances where the current model performs poorly.\n",
    "\n",
    "### 3. **Updating Predictions:**\n",
    "   - The predictions of the weak learner are scaled by a learning rate (shrinkage) and added to the current model's predictions.\n",
    "   - This update is intended to move the model closer to the true values.\n",
    "\n",
    "### 4. **Iterative Process:**\n",
    "   - Steps 2 and 3 are repeated for a predefined number of iterations or until a specified level of accuracy is achieved.\n",
    "   - In each iteration, a new weak learner is trained to correct the errors of the current ensemble.\n",
    "\n",
    "### 5. **Final Prediction:**\n",
    "   - The final prediction is the sum of the initial prediction and the weighted sum of the weak learner predictions.\n",
    "   - The learning rate controls the contribution of each weak learner to the final prediction.\n",
    "\n",
    "### Key Concepts:\n",
    "\n",
    "- **Residuals:**\n",
    "  - Gradient Boosting focuses on minimizing the residuals, which are the differences between the actual and predicted values.\n",
    "\n",
    "- **Learning Rate:**\n",
    "  - The learning rate (or shrinkage) is a hyperparameter that controls the step size in the direction of minimizing the residuals. Smaller values require more weak learners for the same level of performance but can improve model robustness.\n",
    "\n",
    "- **Loss Function:**\n",
    "  - The algorithm minimizes a loss function during training, often the mean squared error for regression tasks. The weak learners are fit to the negative gradient of the loss function.\n",
    "\n",
    "- **Trees as Weak Learners:**\n",
    "  - Decision trees are commonly used as weak learners. They are usually shallow trees to prevent overfitting.\n",
    "\n",
    "### Popular Implementations:\n",
    "\n",
    "- **Scikit-learn's GradientBoostingRegressor:**\n",
    "  - Scikit-learn provides an implementation of Gradient Boosting Regression in the `GradientBoostingRegressor` class.\n",
    "\n",
    "- **XGBoost (Extreme Gradient Boosting):**\n",
    "  - XGBoost is a popular and efficient library for gradient boosting, known for its speed and performance. It is widely used in competitions and data science projects.\n",
    "\n",
    "- **LightGBM and CatBoost:**\n",
    "  - LightGBM and CatBoost are other gradient boosting libraries that offer efficient implementations with additional features like handling categorical variables seamlessly.\n",
    "\n",
    "Gradient Boosting Regression is a powerful technique known for its accuracy and ability to handle complex relationships in data. It is widely used in various regression tasks, including predicting stock prices, house prices, and other continuous variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2. \n",
    "### Implement a simple gradient boosting algorithm from scratch using Python and NumPy. Use a simple regression problem as an example and train the model on a small dataset. Evaluate the model's performance using metrics such as mean squared error and R-squared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "class GradientBoostingRegressor:\n",
    "    def __init__(self, n_estimators=100, learning_rate=0.1):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.models = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Initialize with the mean of the target variable\n",
    "        initial_prediction = np.mean(y)\n",
    "        predictions = np.full_like(y, initial_prediction)\n",
    "\n",
    "        for _ in range(self.n_estimators):\n",
    "            # Compute residuals\n",
    "            residuals = y - predictions\n",
    "\n",
    "            # Fit a weak learner (decision tree) to the residuals\n",
    "            model = DecisionTreeRegressor(max_depth=3)\n",
    "            model.fit(X, residuals)\n",
    "\n",
    "            # Make predictions with the weak learner\n",
    "            weak_learner_predictions = model.predict(X)\n",
    "\n",
    "            # Update the ensemble's predictions\n",
    "            predictions += self.learning_rate * weak_learner_predictions\n",
    "\n",
    "            # Store the weak learner in the ensemble\n",
    "            self.models.append(model)\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Make predictions with the ensemble of weak learners\n",
    "        predictions = np.zeros(X.shape[0])\n",
    "\n",
    "        for model in self.models:\n",
    "            predictions += self.learning_rate * model.predict(X)\n",
    "\n",
    "        return predictions\n",
    "\n",
    "# Example usage:\n",
    "# Assuming X_train, y_train, X_test, y_test are your training and testing data\n",
    "X_train = np.array([[1], [2], [3], [4], [5]])\n",
    "y_train = np.array([2, 4, 5, 4, 5])\n",
    "\n",
    "X_test = np.array([[6], [7]])\n",
    "y_test = np.array([6, 7])\n",
    "\n",
    "# Initialize and train the model\n",
    "model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "r2 = r2_score(y_test, predictions)\n",
    "\n",
    "print(f\"Mean Squared Error: {mse:.2f}\")\n",
    "print(f\"R-squared: {r2:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3. \n",
    "#### Experiment with different hyperparameters such as learning rate, number of trees, and tree depth to optimise the performance of the model. Use grid search or random search to find the best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Example data\n",
    "np.random.seed(42)\n",
    "X = np.random.rand(100, 1) * 10\n",
    "y = 3 * X.squeeze() + np.random.randn(100) * 2\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the parameter grid for grid search\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 4, 5]\n",
    "}\n",
    "\n",
    "# Create the GradientBoostingRegressor\n",
    "gb_regressor = GradientBoostingRegressor()\n",
    "\n",
    "# Create GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=gb_regressor, param_grid=param_grid, scoring='neg_mean_squared_error', cv=5)\n",
    "\n",
    "# Fit the model to the training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# Print the best parameters\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "\n",
    "# Make predictions on the test set using the best model\n",
    "best_model = grid_search.best_estimator_\n",
    "predictions = best_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "r2 = r2_score(y_test, predictions)\n",
    "\n",
    "print(f\"\\nMean Squared Error: {mse:.2f}\")\n",
    "print(f\"R-squared: {r2:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q4.\n",
    "### What is a weak learner in Gradient Boosting?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the context of Gradient Boosting, a weak learner refers to a model that performs slightly better than random chance on a given task. Specifically, it is a model that has low predictive power on its own but can be systematically improved through the boosting process.\n",
    "\n",
    "Typically, decision trees with limited depth are used as weak learners in gradient boosting algorithms. These decision trees are often referred to as \"stumps\" when they are shallow, consisting of only a few nodes or levels. The use of shallow trees helps prevent overfitting, as each tree is constrained to capture only simple relationships in the data.\n",
    "\n",
    "The concept of a weak learner is fundamental to the success of boosting algorithms. In the boosting process, weak learners are trained sequentially, and each subsequent learner focuses on the mistakes or errors made by the combined ensemble of the existing learners. The idea is to leverage the strengths of multiple weak learners to create a strong and accurate predictive model.\n",
    "\n",
    "Characteristics of a Weak Learner in Gradient Boosting:\n",
    "\n",
    "1. **Low Complexity:**\n",
    "   - Weak learners are intentionally kept simple, with limited complexity. Shallow decision trees are a common choice to ensure simplicity.\n",
    "\n",
    "2. **Slightly Better Than Random Guessing:**\n",
    "   - A weak learner should perform just above random chance on the task at hand. It doesn't need to be highly accurate on its own.\n",
    "\n",
    "3. **Captures Local Patterns:**\n",
    "   - The weak learner captures local patterns or relationships in the data, focusing on the most apparent features.\n",
    "\n",
    "4. **Emphasizes Misclassified Instances:**\n",
    "   - In the boosting process, the weak learner gives more attention to instances that were misclassified by the existing ensemble, helping the overall model improve its performance.\n",
    "\n",
    "5. **Ensemble Building Block:**\n",
    "   - While individually weak, the collective strength of multiple weak learners, appropriately weighted and combined, results in a strong and accurate ensemble model.\n",
    "\n",
    "Gradient boosting algorithms, such as AdaBoost, XGBoost, and LightGBM, use the weak learner concept as a foundational element. The boosting process ensures that the ensemble of weak learners gradually adapts and improves its predictive power, ultimately leading to a powerful and accurate model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q5.\n",
    "### What is the intuition behind the Gradient Boosting algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The intuition behind the Gradient Boosting algorithm can be understood by breaking down its key principles and steps. Gradient Boosting is an ensemble learning technique that builds a strong predictive model by combining the predictions of multiple weak learners, typically shallow decision trees. Here's the intuition behind Gradient Boosting:\n",
    "\n",
    "### 1. **Start with a Simple Model:**\n",
    "   - The algorithm begins with a simple model that serves as the initial approximation of the target variable. For regression tasks, this initial prediction is often set to the mean of the target variable.\n",
    "\n",
    "### 2. **Focus on Residuals (Errors):**\n",
    "   - The first weak learner (tree) is trained to predict the residuals, which are the differences between the actual values and the initial prediction. This means it focuses on capturing the errors made by the initial model.\n",
    "\n",
    "### 3. **Sequential Model Building:**\n",
    "   - Subsequent weak learners are trained sequentially, with each one addressing the residuals left by the combination of the existing ensemble. Each new model is fitted to the negative gradient (partial derivative) of the loss function with respect to the current ensemble's predictions.\n",
    "\n",
    "### 4. **Combining Weak Learners:**\n",
    "   - The predictions of all weak learners are combined to produce the final ensemble prediction. The contribution of each weak learner is determined by a learning rate, which scales the impact of each individual model on the final prediction.\n",
    "\n",
    "### 5. **Adaptive Learning:**\n",
    "   - The learning process is adaptive, with each new weak learner focusing on the mistakes of the current ensemble. Instances that were difficult to predict or had large residuals are given higher importance in subsequent iterations.\n",
    "\n",
    "### 6. **Regularization and Shrinkage:**\n",
    "   - Gradient Boosting includes a regularization term and a shrinkage parameter (learning rate) to control the complexity of the final model. Smaller learning rates lead to more gradual updates and better generalization.\n",
    "\n",
    "### 7. **Avoid Overfitting with Tree Constraints:**\n",
    "   - To prevent overfitting, each weak learner (tree) is typically constrained by limiting its depth. Shallow trees are preferred to capture simple relationships and avoid memorizing noise in the data.\n",
    "\n",
    "### 8. **Ensemble's Strength from Weakness:**\n",
    "   - The strength of Gradient Boosting comes from combining the predictions of individually weak learners. The ensemble has the capacity to capture complex patterns and relationships in the data, despite each weak learner's simplicity.\n",
    "\n",
    "### Key Intuitive Aspects:\n",
    "\n",
    "- **Sequential Correction of Errors:**\n",
    "  - Each new weak learner corrects the errors made by the existing ensemble, leading to a gradual improvement in predictions.\n",
    "\n",
    "- **Adaptation to Data Complexity:**\n",
    "  - The algorithm adapts to the complexity of the data by assigning higher weights to instances with larger residuals.\n",
    "\n",
    "- **Ensemble Learning for Robustness:**\n",
    "  - The ensemble approach mitigates the risk of overfitting and improves the model's robustness by combining diverse weak learners.\n",
    "\n",
    "In summary, Gradient Boosting iteratively builds an ensemble of weak learners, with each learner focusing on the mistakes of the existing ensemble. The final model, a weighted sum of these weak learners, demonstrates high predictive power and robustness. The intuition lies in the algorithm's ability to adapt, learn from errors, and combine the strengths of simple models to create a sophisticated and accurate predictive model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q6.\n",
    "### How does Gradient Boosting algorithm build an ensemble of weak learners?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Gradient Boosting algorithm builds an ensemble of weak learners sequentially, with each new learner correcting the errors made by the existing ensemble. The process involves the following steps:\n",
    "\n",
    "### 1. **Initialization:**\n",
    "   - Start with an initial approximation for the target variable. For regression tasks, this is often the mean of the target variable.\n",
    "\n",
    "### 2. **Compute Residuals (Errors):**\n",
    "   - Calculate the residuals by subtracting the current predictions from the actual target values. These residuals represent the errors made by the current ensemble.\n",
    "\n",
    "### 3. **Train a Weak Learner:**\n",
    "   - Fit a weak learner (typically a shallow decision tree) to the residuals. The goal is to find a model that captures the patterns in the data not captured by the existing ensemble.\n",
    "\n",
    "### 4. **Update Predictions:**\n",
    "   - Update the predictions by adding the output of the weak learner, scaled by a learning rate, to the current predictions. The learning rate controls the contribution of each weak learner to the ensemble.\n",
    "\n",
    "   \\[ \\text{New Predictions} = \\text{Current Predictions} + \\text{Learning Rate} \\times \\text{Weak Learner Output} \\]\n",
    "\n",
    "### 5. **Compute New Residuals:**\n",
    "   - Calculate the new residuals by subtracting the updated predictions from the actual target values.\n",
    "\n",
    "### 6. **Repeat:**\n",
    "   - Repeat steps 3-5 for a predefined number of iterations or until a specified level of accuracy is achieved.\n",
    "\n",
    "### 7. **Final Ensemble:**\n",
    "   - The final ensemble is the sum of all weak learners' predictions, each multiplied by its learning rate.\n",
    "\n",
    "   \\[ \\text{Final Prediction} = \\text{Initial Approximation} + \\sum_{i=1}^{N} \\text{Learning Rate} \\times \\text{Weak Learner}_i \\]\n",
    "\n",
    "### Key Points:\n",
    "\n",
    "- **Sequential Correction of Errors:**\n",
    "  - Each new weak learner focuses on the errors (residuals) made by the current ensemble, attempting to correct them and improve the overall predictions.\n",
    "\n",
    "- **Adaptive Learning:**\n",
    "  - The learning process is adaptive, with each new weak learner emphasizing instances that were difficult to predict by assigning higher weights to their residuals.\n",
    "\n",
    "- **Combining Weak Models:**\n",
    "  - The final ensemble combines the predictions of all weak learners, with each model contributing to the overall prediction based on its learning rate.\n",
    "\n",
    "- **Regularization:**\n",
    "  - Gradient Boosting includes regularization terms to control the complexity of the final model, preventing overfitting.\n",
    "\n",
    "- **Shallow Trees:**\n",
    "  - Decision trees used as weak learners are typically shallow to capture simple relationships and avoid overfitting.\n",
    "\n",
    "By iteratively training weak learners on the residuals of the previous ensemble, Gradient Boosting builds a strong ensemble model capable of capturing complex patterns and achieving high predictive accuracy. The sequential nature of the process, along with adaptive learning and regularization, contributes to the effectiveness of the algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q7. \n",
    "### What are the steps involved in constructing the mathematical intuition of Gradient Boosting algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Constructing the mathematical intuition behind the Gradient Boosting algorithm involves understanding the underlying principles and formulas. Let's break down the key mathematical steps and intuition:\n",
    "\n",
    "### 1. **Objective Function (Loss Function):**\n",
    "   - Define a loss function \\(L(y, F(x))\\) that measures the difference between the true values \\(y\\) and the current model's predictions \\(F(x)\\). Common choices for regression tasks include the mean squared error (MSE) or absolute error.\n",
    "\n",
    "### 2. **Start with an Initial Model:**\n",
    "   - Initialize the model with a constant value, often the mean of the target variable. The initial prediction is denoted as \\(F_0(x)\\).\n",
    "\n",
    "### 3. **Compute Residuals:**\n",
    "   - Calculate the residuals (errors) by subtracting the current predictions from the true values:\n",
    "     \\[ \\text{Residuals} = y - F_0(x) \\]\n",
    "\n",
    "### 4. **Build Weak Learners (Decision Trees):**\n",
    "   - Train a weak learner (e.g., shallow decision tree) on the residuals. Let the weak learner be denoted as \\(h_i(x)\\).\n",
    "\n",
    "### 5. **Compute the Negative Gradient of the Loss Function:**\n",
    "   - Compute the negative gradient of the loss function with respect to the current model's predictions:\n",
    "     \\[ -\\frac{\\partial L(y, F(x))}{\\partial F(x)} \\]\n",
    "\n",
    "### 6. **Fit Weak Learner to Negative Gradient:**\n",
    "   - Fit the weak learner to the negative gradient, effectively modeling the direction in which the current model needs correction.\n",
    "\n",
    "### 7. **Update Model Predictions:**\n",
    "   - Update the model predictions by adding the output of the weak learner, scaled by a learning rate (\\(\\alpha\\)):\n",
    "     \\[ F_{i+1}(x) = F_i(x) + \\alpha h_i(x) \\]\n",
    "\n",
    "### 8. **Repeat for Multiple Iterations:**\n",
    "   - Repeat steps 3-7 for a predefined number of iterations or until a specified level of accuracy is achieved.\n",
    "\n",
    "### 9. **Final Ensemble:**\n",
    "   - The final ensemble is the sum of all weak learners, each weighted by its learning rate:\n",
    "     \\[ F(x) = F_0(x) + \\alpha_1 h_1(x) + \\alpha_2 h_2(x) + \\ldots + \\alpha_N h_N(x) \\]\n",
    "\n",
    "### Key Intuition:\n",
    "\n",
    "- **Error Correction:**\n",
    "  - Each weak learner is trained to correct the errors (residuals) made by the current ensemble.\n",
    "\n",
    "- **Negative Gradient:**\n",
    "  - The weak learner is fit to the negative gradient of the loss function, ensuring it moves in the direction that reduces the loss.\n",
    "\n",
    "- **Sequential Learning:**\n",
    "  - The process is sequential, with each new weak learner building upon the corrections made by the previous models.\n",
    "\n",
    "- **Adaptive Learning Rates:**\n",
    "  - The learning rate (\\(\\alpha\\)) controls the contribution of each weak learner. Smaller values emphasize a gradual learning process, preventing overfitting.\n",
    "\n",
    "- **Ensemble Adaptation:**\n",
    "  - The final ensemble is an adaptive combination of weak learners, effectively capturing complex relationships in the data.\n",
    "\n",
    "Understanding these mathematical steps provides insight into how Gradient Boosting optimizes the model by iteratively focusing on the mistakes of the current ensemble. The algorithm adapts to the data, corrects errors, and builds a strong predictive model through the ensemble of weak learners."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Completed_17th_April_Assignment:\n",
    "## _______________________________"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
