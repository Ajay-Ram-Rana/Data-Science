{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1.\n",
    "### What are the different types of clustering algorithms, and how do they differ in terms of their approach and underlying assumptions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clustering is a type of unsupervised learning where the goal is to group similar data points into clusters. There are various clustering algorithms, each with its own approach and underlying assumptions. Here are some common types of clustering algorithms:\n",
    "\n",
    "1. **K-Means Clustering:**\n",
    "   - **Approach:** Divides the data into k clusters based on the mean of data points in each cluster.\n",
    "   - **Assumptions:** Assumes clusters are spherical and of similar size. It minimizes the within-cluster variance.\n",
    "\n",
    "2. **Hierarchical Clustering:**\n",
    "   - **Approach:** Builds a tree-like hierarchy of clusters. Can be agglomerative (start with individual points and merge) or divisive (start with one cluster and split).\n",
    "   - **Assumptions:** Does not assume a particular number of clusters, and the structure is determined based on the data.\n",
    "\n",
    "3. **DBSCAN (Density-Based Spatial Clustering of Applications with Noise):**\n",
    "   - **Approach:** Groups together data points that are close to each other and have a sufficient number of neighbors, defining clusters based on density.\n",
    "   - **Assumptions:** Assumes that clusters are dense and separated by areas of lower point density.\n",
    "\n",
    "4. **Mean Shift:**\n",
    "   - **Approach:** Shifts the center of mass of data points towards the regions of higher point density.\n",
    "   - **Assumptions:** Assumes that clusters are areas of high point density separated by areas of low point density.\n",
    "\n",
    "5. **Agglomerative Clustering:**\n",
    "   - **Approach:** Starts with individual data points as clusters and iteratively merges the closest clusters until a stopping criterion is met.\n",
    "   - **Assumptions:** Hierarchical in nature, does not assume a specific number of clusters.\n",
    "\n",
    "6. **Gaussian Mixture Models (GMM):**\n",
    "   - **Approach:** Models the data as a mixture of Gaussian distributions, allowing for probabilistic assignment of data points to clusters.\n",
    "   - **Assumptions:** Assumes that the data is generated from a mixture of several Gaussian distributions.\n",
    "\n",
    "7. **Self-Organizing Maps (SOM):**\n",
    "   - **Approach:** Utilizes a neural network to map high-dimensional data onto a lower-dimensional grid, preserving the topological properties of the input space.\n",
    "   - **Assumptions:** Primarily used for visualization, no strict assumptions about cluster shapes.\n",
    "\n",
    "8. **Fuzzy C-Means (FCM):**\n",
    "   - **Approach:** Similar to K-Means but allows data points to belong to multiple clusters with varying degrees of membership.\n",
    "   - **Assumptions:** Assumes that data points can belong to multiple clusters simultaneously.\n",
    "\n",
    "9. **OPTICS (Ordering Points To Identify the Clustering Structure):**\n",
    "   - **Approach:** Ranks the data points based on their density and connectivity, allowing for the identification of clusters with varying shapes and sizes.\n",
    "   - **Assumptions:** More flexible in handling clusters of different densities.\n",
    "\n",
    "10. **Spectral Clustering:**\n",
    "   - **Approach:** Utilizes the eigenvalues of the similarity matrix to reduce the dimensionality of the data, making it easier to identify clusters.\n",
    "   - **Assumptions:** Can handle non-convex clusters and is effective in capturing complex structures.\n",
    "\n",
    "The choice of a clustering algorithm depends on the nature of the data, the desired cluster shapes, and the specific goals of the analysis. It's important to consider the assumptions and limitations of each algorithm when applying them to real-world datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2.\n",
    "### What is K-means clustering, and how does it work?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-Means clustering is a popular unsupervised machine learning algorithm used for partitioning a dataset into K distinct, non-overlapping subsets (clusters). The goal is to group similar data points together and assign them to clusters based on certain features. The algorithm is widely used for various applications, such as image segmentation, document categorization, and customer segmentation.\n",
    "\n",
    "Here's a step-by-step explanation of how K-Means clustering works:\n",
    "\n",
    "1. **Initialization:**\n",
    "   - Choose the number of clusters, K, that you want to identify in the dataset.\n",
    "   - Randomly initialize K cluster centroids. These centroids represent the initial guesses for the centers of the clusters.\n",
    "\n",
    "2. **Assignment Step:**\n",
    "   - For each data point, calculate the distance to each of the K centroids. Common distance metrics include Euclidean distance or Manhattan distance.\n",
    "   - Assign each data point to the cluster whose centroid is the closest.\n",
    "\n",
    "3. **Update Step:**\n",
    "   - Recalculate the centroids of the clusters based on the mean of the data points assigned to each cluster.\n",
    "   - The new centroid is the center of gravity or mean of all the points in the cluster.\n",
    "\n",
    "4. **Repeat:**\n",
    "   - Repeat the assignment and update steps iteratively until convergence. Convergence occurs when the centroids no longer change significantly or a predefined number of iterations is reached.\n",
    "\n",
    "5. **Result:**\n",
    "   - The final result is K clusters, each with its centroid.\n",
    "   - Each data point is assigned to the cluster whose centroid is closest.\n",
    "\n",
    "It's important to note some key aspects of K-Means clustering:\n",
    "\n",
    "- **Number of Clusters (K):** The choice of the number of clusters (K) is crucial. It can impact the quality of the clustering. Different methods, such as the elbow method or silhouette analysis, can be used to determine an optimal K.\n",
    "\n",
    "- **Initialization Sensitivity:** K-Means is sensitive to the initial placement of centroids. Different initializations may lead to different final cluster assignments. To mitigate this, multiple runs with different initializations are often performed, and the best result is selected.\n",
    "\n",
    "- **Euclidean Distance:** K-Means relies on the Euclidean distance metric, which assumes clusters are spherical and equally sized. This can be a limitation when dealing with non-spherical or unevenly sized clusters.\n",
    "\n",
    "- **Scalability:** K-Means is computationally efficient and scalable, making it suitable for large datasets.\n",
    "\n",
    "Despite its simplicity and efficiency, K-Means may not perform well in all situations, especially when dealing with clusters of different shapes, densities, or sizes. It's essential to consider the characteristics of the data and potentially explore other clustering algorithms based on the specific requirements of the task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3. \n",
    "### What are some advantages and limitations of K-means clustering compared to other clustering techniques?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advantages of K-Means Clustering:\n",
    "\n",
    "1. **Simplicity and Efficiency:**\n",
    "   - K-Means is straightforward to implement and computationally efficient, making it suitable for large datasets and real-time applications.\n",
    "\n",
    "2. **Scalability:**\n",
    "   - The algorithm scales well with the number of data points, making it efficient for clustering in large datasets.\n",
    "\n",
    "3. **Versatility:**\n",
    "   - K-Means can be applied to a wide range of data types, including numerical, categorical, and mixed data.\n",
    "\n",
    "4. **Ease of Interpretation:**\n",
    "   - The results of K-Means are easy to interpret. Each data point is assigned to a cluster, and the cluster centroids provide a clear representation of the cluster's center.\n",
    "\n",
    "5. **Convergence:**\n",
    "   - K-Means typically converges to a solution, and the convergence is relatively fast compared to some other clustering algorithms.\n",
    "\n",
    "6. **Applicability to Balanced Clusters:**\n",
    "   - K-Means performs well when clusters are approximately spherical, equally sized, and have similar densities.\n",
    "\n",
    "### Limitations of K-Means Clustering:\n",
    "\n",
    "1. **Sensitivity to Initialization:**\n",
    "   - The final clustering outcome can depend on the initial placement of centroids, which may result in different local optima. Multiple runs with different initializations are often recommended.\n",
    "\n",
    "2. **Assumption of Equal Variance:**\n",
    "   - K-Means assumes that clusters have equal variances, which may not be the case in real-world data where clusters can have different shapes, sizes, and variances.\n",
    "\n",
    "3. **Difficulty with Non-Globular Clusters:**\n",
    "   - K-Means struggles when dealing with clusters that are non-spherical or have complex shapes. It may produce suboptimal results in such cases.\n",
    "\n",
    "4. **Fixed Number of Clusters:**\n",
    "   - The user must specify the number of clusters (K) in advance, which can be a challenge, especially when the true number of clusters is unknown.\n",
    "\n",
    "5. **Sensitive to Outliers:**\n",
    "   - Outliers can significantly impact the performance of K-Means. Since the algorithm minimizes the sum of squared distances, outliers can distort cluster centroids and affect the results.\n",
    "\n",
    "6. **Metric Dependency:**\n",
    "   - The choice of distance metric, typically Euclidean distance, may not be appropriate for all types of data. The algorithm's performance is influenced by the metric used.\n",
    "\n",
    "7. **Not Suitable for Unevenly Sized Clusters:**\n",
    "   - K-Means may not perform well when dealing with clusters of different sizes and densities. It tends to produce clusters with approximately equal sizes.\n",
    "\n",
    "8. **May Not Find Global Optima:**\n",
    "   - Due to its iterative nature, K-Means may converge to a local optimum, and there is no guarantee that the global optimum is reached.\n",
    "\n",
    "9. **Binary Assignments:**\n",
    "   - Each data point is strictly assigned to a single cluster, even if it may belong to multiple clusters with varying degrees of membership. This limitation can be addressed by fuzzy clustering techniques.\n",
    "\n",
    "10. **Sensitive to Feature Scaling:**\n",
    "    - The algorithm is sensitive to the scale of features, and it's advisable to normalize or standardize the data before applying K-Means.\n",
    "\n",
    "While K-Means has its advantages, it's essential to consider the characteristics of the data and the specific requirements of the clustering task. Depending on the nature of the data, other clustering algorithms with different assumptions and capabilities might be more suitable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q4. \n",
    "### How do you determine the optimal number of clusters in K-means clustering, and what are some common methods for doing so?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determining the optimal number of clusters (K) in K-Means clustering is a crucial step, and various methods can be employed for this purpose. Here are some common methods:\n",
    "\n",
    "1. **Elbow Method:**\n",
    "   - Plot the sum of squared distances (inertia) from each point to its assigned cluster centroid for different values of K.\n",
    "   - Look for an \"elbow\" point in the plot where the rate of decrease in inertia sharply changes. The point where adding more clusters provides diminishing returns is often considered the optimal K.\n",
    "\n",
    "2. **Silhouette Score:**\n",
    "   - Calculate the silhouette score for different values of K. The silhouette score measures how well-separated the clusters are.\n",
    "   - Choose the K that maximizes the silhouette score. A higher silhouette score indicates better-defined clusters.\n",
    "\n",
    "3. **Gap Statistics:**\n",
    "   - Compare the within-cluster sum of squared distances for the actual data with the sum of squared distances for a random reference distribution.\n",
    "   - The optimal K is where the gap between the actual data's performance and the reference distribution is the largest.\n",
    "\n",
    "4. **Davies-Bouldin Index:**\n",
    "   - Evaluate clustering quality by considering the compactness and separation of clusters.\n",
    "   - Choose the K that minimizes the Davies-Bouldin index, where lower values indicate better clustering.\n",
    "\n",
    "5. **Calinski-Harabasz Index:**\n",
    "   - Evaluate clustering quality based on the ratio of the between-cluster variance to within-cluster variance.\n",
    "   - Select the K that maximizes the Calinski-Harabasz index.\n",
    "\n",
    "6. **Cross-Validation:**\n",
    "   - Use cross-validation to evaluate the performance of the K-Means algorithm for different values of K.\n",
    "   - Choose the K that maximizes the clustering performance on the validation set.\n",
    "\n",
    "7. **Gap Statistic:**\n",
    "   - Similar to the Gap Statistics method, the Gap Statistic compares the performance of the clustering algorithm on the actual data with its performance on random data.\n",
    "   - The optimal K is the one that maximizes the gap between the actual data's performance and the expected performance on random data.\n",
    "\n",
    "8. **Information Criterion (e.g., AIC, BIC):**\n",
    "   - Apply information criteria such as Akaike Information Criterion (AIC) or Bayesian Information Criterion (BIC) to evaluate the trade-off between model complexity and fit.\n",
    "   - Choose the K that minimizes the information criterion.\n",
    "\n",
    "9. **Visual Inspection:**\n",
    "   - Visualize the data and the results of clustering for different values of K. Assess the results based on domain knowledge and the meaningfulness of the clusters.\n",
    "   - Sometimes, the visual inspection of clustering results can provide insights into the appropriate number of clusters.\n",
    "\n",
    "It's important to note that no single method is universally superior, and different methods may suggest different values of K. It's often recommended to use a combination of these methods and consider the specific characteristics of the data and the goals of the analysis when determining the optimal number of clusters. Additionally, exploring the stability of clustering results across multiple runs with different initializations can enhance confidence in the chosen K."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q5. \n",
    "### What are some applications of K-means clustering in real-world scenarios, and how has it been used to solve specific problems?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-Means clustering is a versatile algorithm with applications across various domains. Here are some real-world scenarios where K-Means clustering has been applied to solve specific problems:\n",
    "\n",
    "1. **Customer Segmentation:**\n",
    "   - **Application:** In marketing, businesses use K-Means to segment customers based on their purchasing behavior, demographics, or other relevant features.\n",
    "   - **Benefits:** This helps companies tailor marketing strategies for different customer segments, improving the efficiency of targeted campaigns.\n",
    "\n",
    "2. **Image Compression and Segmentation:**\n",
    "   - **Application:** In computer vision, K-Means can be used to compress images by reducing the number of colors. It is also applied for image segmentation, where pixels with similar colors are grouped together.\n",
    "   - **Benefits:** Image compression reduces storage space, and segmentation is useful for object recognition and analysis.\n",
    "\n",
    "3. **Anomaly Detection:**\n",
    "   - **Application:** K-Means can identify anomalies or outliers in datasets by treating normal data points as one cluster and outliers as separate clusters.\n",
    "   - **Benefits:** This is useful in fraud detection, network security, and quality control where detecting unusual patterns is crucial.\n",
    "\n",
    "4. **Document Clustering:**\n",
    "   - **Application:** In natural language processing (NLP), K-Means can be used to cluster documents based on their content, enabling topic modeling and document organization.\n",
    "   - **Benefits:** Helps in organizing and summarizing large document collections for improved information retrieval.\n",
    "\n",
    "5. **Genomic Data Analysis:**\n",
    "   - **Application:** In bioinformatics, K-Means clustering is applied to analyze gene expression data, identifying patterns and grouping genes with similar expression profiles.\n",
    "   - **Benefits:** This aids in understanding genetic relationships, identifying biomarkers, and classifying different types of diseases.\n",
    "\n",
    "6. **Retail Inventory Management:**\n",
    "   - **Application:** Retailers use K-Means to cluster products based on sales patterns, allowing for optimized inventory management and stock replenishment strategies.\n",
    "   - **Benefits:** Helps in minimizing stockouts, reducing excess inventory, and improving overall supply chain efficiency.\n",
    "\n",
    "7. **Social Media Analysis:**\n",
    "   - **Application:** K-Means can be applied to cluster users or content on social media platforms based on user behavior, preferences, or content similarity.\n",
    "   - **Benefits:** Enhances targeted advertising, recommendation systems, and personalized content delivery.\n",
    "\n",
    "8. **Climate Pattern Recognition:**\n",
    "   - **Application:** K-Means clustering is used in environmental science to identify climate patterns based on meteorological data.\n",
    "   - **Benefits:** Helps in understanding regional climate variations, predicting extreme weather events, and informing environmental policies.\n",
    "\n",
    "9. **Network Analysis:**\n",
    "   - **Application:** K-Means clustering can be applied to group nodes in a network based on connectivity patterns.\n",
    "   - **Benefits:** Useful in identifying communities within social networks, analyzing communication patterns, and improving network structure.\n",
    "\n",
    "10. **Medical Imaging:**\n",
    "    - **Application:** In medical image analysis, K-Means can be used to segment medical images based on pixel intensity or other features.\n",
    "    - **Benefits:** Facilitates the identification of specific structures or anomalies in medical images, aiding in diagnosis and treatment planning.\n",
    "\n",
    "These examples illustrate the versatility of K-Means clustering in solving diverse real-world problems across different domains. The algorithm's simplicity and effectiveness make it a valuable tool for exploratory data analysis and pattern recognition in various applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q6.\n",
    "### How do you interpret the output of a K-means clustering algorithm, and what insights can you derive from the resulting clusters?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpreting the output of a K-Means clustering algorithm involves understanding the characteristics of each cluster and the relationships between clusters. Here are key steps and insights you can derive from the resulting clusters:\n",
    "\n",
    "1. **Cluster Centers (Centroids):**\n",
    "   - Examine the coordinates of the cluster centers (centroids). These represent the mean values of the features for each cluster.\n",
    "   - Insights: Identify the central tendencies of each cluster and understand the average values of the features within them.\n",
    "\n",
    "2. **Cluster Sizes:**\n",
    "   - Explore the sizes of each cluster, i.e., the number of data points assigned to each cluster.\n",
    "   - Insights: Understand the distribution of data points among clusters. Unequal cluster sizes may indicate the presence of dominant patterns.\n",
    "\n",
    "3. **Visualization:**\n",
    "   - Visualize the clusters in a scatter plot or other suitable visualization methods.\n",
    "   - Insights: Assess the spatial distribution of data points and how well the clusters are separated. Visualization can reveal patterns and relationships.\n",
    "\n",
    "4. **Feature Importance:**\n",
    "   - Analyze the importance of features in distinguishing between clusters. Consider the within-cluster variance and between-cluster variance for each feature.\n",
    "   - Insights: Identify features that significantly contribute to the differentiation of clusters. These features can provide insights into the factors driving cluster formation.\n",
    "\n",
    "5. **Comparison Across Clusters:**\n",
    "   - Conduct statistical tests or exploratory analysis to compare means, variances, or other relevant statistics across different clusters.\n",
    "   - Insights: Identify significant differences or similarities between clusters, helping to understand the unique characteristics of each group.\n",
    "\n",
    "6. **Domain Knowledge Integration:**\n",
    "   - Incorporate domain knowledge to interpret the meaning of the clusters. Consider the context of the data and how the identified patterns align with existing knowledge.\n",
    "   - Insights: Relate cluster characteristics to domain-specific phenomena, facilitating a more meaningful interpretation.\n",
    "\n",
    "7. **Validation Metrics:**\n",
    "   - If applicable, use validation metrics such as silhouette score, Davies-Bouldin index, or others to assess the quality of clustering.\n",
    "   - Insights: Evaluate the overall effectiveness of the clustering solution. Higher silhouette scores indicate well-separated clusters.\n",
    "\n",
    "8. **Outliers and Noise:**\n",
    "   - Identify any outliers or noise that may not fit well into clusters.\n",
    "   - Insights: Understand whether outliers represent anomalies or errors in the data, and decide whether to include or exclude them in further analyses.\n",
    "\n",
    "9. **Temporal Analysis (if applicable):**\n",
    "   - If the data has a temporal aspect, analyze how clusters evolve over time.\n",
    "   - Insights: Identify trends, shifts, or patterns in the temporal evolution of clusters, providing insights into changing dynamics.\n",
    "\n",
    "10. **Iterative Refinement:**\n",
    "    - If necessary, perform iterative refinement by adjusting the number of clusters, re-running the algorithm, and reassessing the results.\n",
    "    - Insights: Explore how changes in the number of clusters affect the interpretability and stability of the solution.\n",
    "\n",
    "By systematically examining these aspects, you can gain valuable insights into the structure of your data and the meaningful patterns captured by the K-Means clustering algorithm. Interpretation often involves a combination of statistical analysis, visualization, and domain-specific knowledge to ensure a comprehensive understanding of the clustered data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q7.\n",
    "### What are some common challenges in implementing K-means clustering, and how can you address them?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing K-Means clustering can encounter various challenges, and it's essential to be aware of these issues to ensure a robust and meaningful analysis. Here are some common challenges and strategies to address them:\n",
    "\n",
    "1. **Sensitivity to Initial Centroid Positions:**\n",
    "   - **Challenge:** K-Means is sensitive to the initial placement of centroids, and different initializations can lead to different results.\n",
    "   - **Solution:** Perform multiple runs with different random initializations and choose the clustering solution with the lowest sum of squared distances or other relevant validation metrics.\n",
    "\n",
    "2. **Choosing the Number of Clusters (K):**\n",
    "   - **Challenge:** Selecting the optimal number of clusters (K) is often subjective and challenging.\n",
    "   - **Solution:** Use methods like the elbow method, silhouette score, cross-validation, or other clustering validation metrics to determine the most appropriate value of K. Experiment with a range of K values and assess the stability of results.\n",
    "\n",
    "3. **Handling Outliers:**\n",
    "   - **Challenge:** Outliers can significantly impact cluster centroids, leading to suboptimal results.\n",
    "   - **Solution:** Consider preprocessing the data to identify and handle outliers before running K-Means. Techniques like outlier detection or transformation may be applied to mitigate their impact.\n",
    "\n",
    "4. **Non-Spherical or Unequal-Sized Clusters:**\n",
    "   - **Challenge:** K-Means assumes clusters are spherical and of similar size, which may not hold in all cases.\n",
    "   - **Solution:** Explore clustering algorithms that can handle non-spherical clusters, such as DBSCAN or Gaussian Mixture Models (GMM). Alternatively, use feature scaling to address the issue of unequal variances.\n",
    "\n",
    "5. **Scalability:**\n",
    "   - **Challenge:** K-Means may become computationally expensive for very large datasets.\n",
    "   - **Solution:** Consider using a random subset of the data for initial analysis or explore variants like Mini-Batch K-Means for large datasets. Additionally, parallelization techniques can be employed for distributed computing.\n",
    "\n",
    "6. **Choosing Appropriate Distance Metric:**\n",
    "   - **Challenge:** The choice of distance metric (e.g., Euclidean, Manhattan) may impact the results, and the default metric might not be suitable for all types of data.\n",
    "   - **Solution:** Experiment with different distance metrics based on the characteristics of the data. Custom distance metrics can be defined for specific applications.\n",
    "\n",
    "7. **Handling Categorical Data:**\n",
    "   - **Challenge:** K-Means is designed for numerical data and may not perform well with categorical features.\n",
    "   - **Solution:** Use techniques like one-hot encoding to convert categorical features into numerical format. Alternatively, explore clustering algorithms specifically designed for categorical data.\n",
    "\n",
    "8. **Interpreting Results:**\n",
    "   - **Challenge:** Interpreting the meaning of clusters may be challenging, especially in complex datasets.\n",
    "   - **Solution:** Combine clustering results with domain knowledge, visualize clusters, and perform additional analyses to interpret the practical significance of the identified patterns. Collaboration with domain experts is often crucial.\n",
    "\n",
    "9. **Overfitting:**\n",
    "   - **Challenge:** Overfitting can occur, especially when the number of clusters is too high.\n",
    "   - **Solution:** Regularize the clustering solution by choosing a reasonable number of clusters based on validation metrics. Avoid overfitting by considering the simplicity and interpretability of the model.\n",
    "\n",
    "10. **Handling Missing Data:**\n",
    "    - **Challenge:** K-Means doesn't handle missing data well.\n",
    "    - **Solution:** Impute missing values before running K-Means or consider clustering techniques that can handle missing data more effectively.\n",
    "\n",
    "Being mindful of these challenges and implementing appropriate strategies can contribute to the successful application of K-Means clustering in various scenarios. It's important to iteratively refine the analysis based on insights gained during interpretation and validation steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Completed_27th_April_Assignment:\n",
    "## _______________________________"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
