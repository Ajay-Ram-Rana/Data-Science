{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1. \n",
    "## Explain the assumptions required to use ANOVA and provide examples of violations that could impact the validity of the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis of Variance (ANOVA) is a statistical method used to compare the means of three or more groups to determine whether there are statistically significant differences among them. To use ANOVA effectively, several assumptions must be met. Violations of these assumptions can impact the validity of the results. The key assumptions for ANOVA are as follows:\n",
    "\n",
    "1. Independence: The observations in each group must be independent of each other. This means that the value of one observation should not influence the value of another observation. Violations of this assumption can occur in situations where the data is correlated, such as repeated measures designs.\n",
    "\n",
    "Example of violation: In a study measuring the test scores of students before and after they receive a tutoring program, the same group of students is tested twice. This violates the independence assumption, as the test scores after tutoring may be correlated with the scores before tutoring.\n",
    "\n",
    "2. Normality: The data within each group should be approximately normally distributed. ANOVA is robust to moderate departures from normality, but severe violations can lead to incorrect conclusions.\n",
    "\n",
    "Example of violation: In a study comparing the performance of three different groups of participants in a psychological experiment, the test scores for one group follow a strongly skewed distribution. This violates the normality assumption.\n",
    "\n",
    "3. Homogeneity of Variance (Homoscedasticity): The variances of the groups being compared should be approximately equal. If the variances are not equal, it can lead to a loss of power and an increased risk of Type I errors (incorrectly concluding there is a significant difference when there isn't).\n",
    "\n",
    "Example of violation: In an experiment comparing the strength of three different brands of a material, the variances of the test results for the brands are significantly different. This violates the homogeneity of variance assumption.\n",
    "\n",
    "4. Equal Sample Sizes (for one-way ANOVA): In a one-way ANOVA, it is ideal to have equal sample sizes in each group. However, ANOVA is somewhat robust to unequal sample sizes, especially when the sample sizes are not extremely unbalanced.\n",
    "\n",
    "Example of violation: In a study comparing the effectiveness of three different teaching methods, one group has twice as many participants as the other two groups. While ANOVA can handle unequal sample sizes, extremely imbalanced sample sizes may be problematic.\n",
    "\n",
    "5. Additivity (for two-way or multi-way ANOVA): This assumption states that the effects of different factors (in two-way or multi-way ANOVA) are additive. In other words, there should be no interaction effect between factors.\n",
    "\n",
    "Example of violation: In a two-way ANOVA examining the effects of both a teaching method and gender on test scores, an interaction effect occurs, where the impact of the teaching method on test scores depends on the gender of the participants.\n",
    "\n",
    "Violations of these assumptions can affect the validity of ANOVA results by leading to inaccurate p-values and potentially incorrect conclusions. When assumptions are violated, alternative statistical tests or data transformations may be necessary. Additionally, robustness of ANOVA tests to these assumptions may vary depending on sample size and the extent of the violations. It's important to carefully consider the data and the specific context of the analysis when using ANOVA and to be aware of potential violations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2.\n",
    "## What are the three types of ANOVA, and in what situations would each be used?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis of Variance (ANOVA) is a statistical technique used to compare the means of three or more groups to determine whether there are statistically significant differences among them. There are three primary types of ANOVA, each designed for specific situations:\n",
    "\n",
    "1. One-Way ANOVA:\n",
    "   - Situation: One-way ANOVA is used when you have one categorical independent variable (with three or more levels or groups) and one continuous dependent variable. It assesses whether there are significant differences in the means of the groups.\n",
    "   - Example: A researcher wants to determine if there are significant differences in the test scores of students who received three different types of tutoring (e.g., Group A, Group B, Group C).\n",
    "\n",
    "2. Two-Way ANOVA:\n",
    "   - Situation: Two-way ANOVA is used when you have two categorical independent variables (factors) and one continuous dependent variable. It evaluates the main effects of each factor and any interaction between the factors. In other words, it examines how two factors together impact the dependent variable.\n",
    "   - Example: A researcher is interested in the effects of both gender (male/female) and teaching method (Method X, Method Y) on test scores. Two-way ANOVA assesses the main effects of gender, the main effects of teaching method, and whether there's an interaction between gender and teaching method.\n",
    "\n",
    "3. Three-Way or Multi-Way ANOVA:\n",
    "   - Situation: Three-way ANOVA and higher-order ANOVAs are used when you have three or more independent variables. These can include combinations of categorical and continuous independent variables. Like two-way ANOVA, they assess main effects and interactions among all the factors.\n",
    "   - Example: A researcher wants to investigate the impact of three factors—diet type (low-fat, high-fat), exercise level (sedentary, moderate, active), and age group (young, middle-aged, elderly)—on weight loss. A three-way ANOVA would assess the main effects of each factor and their interactions.\n",
    "\n",
    "In each type of ANOVA, the goal is to determine whether there are statistically significant differences in the means of the groups, and if so, to identify which groups differ from one another. ANOVA provides an overall test statistic and p-value to assess the significance of group differences. If the overall test is significant, post hoc tests (e.g., Tukey's HSD, Bonferroni) or pairwise comparisons can be conducted to pinpoint which specific groups differ significantly from one another.\n",
    "\n",
    "Choosing the appropriate type of ANOVA depends on the research design, the number and type of independent variables, and the specific hypotheses being tested. It's essential to correctly select and apply the ANOVA method to obtain valid and meaningful results in your analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q3.\n",
    "\n",
    "### What is the partitioning of variance in ANOVA, and why is it important to understand this concept?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The partitioning of variance is a fundamental concept in Analysis of Variance (ANOVA), and it involves breaking down the total variance in the data into different components. Understanding this concept is essential for several reasons:\n",
    "\n",
    "1. Decomposing Variance: ANOVA helps decompose the total variance observed in the data into various components, allowing researchers to understand where the variability in the dependent variable comes from. By partitioning the variance, ANOVA clarifies which portion is due to the effects of the independent variables and which portion is due to random or unexplained variation.\n",
    "\n",
    "2. Assessing Group Differences: ANOVA is used to determine whether there are statistically significant differences among groups. Partitioning the variance allows you to assess the contribution of each factor or interaction term to these group differences. In other words, it helps identify which factors are responsible for the observed variations in the dependent variable.\n",
    "\n",
    "3. Hypothesis Testing: ANOVA generates F-statistics and associated p-values to test the null hypothesis that there are no group differences. The partitioning of variance is crucial in calculating these statistics and determining the statistical significance of the effects. This helps researchers make informed decisions about accepting or rejecting their hypotheses.\n",
    "\n",
    "The partitioning of variance in ANOVA typically involves three main components:\n",
    "\n",
    "1. Total Variance (Total Sum of Squares, SST): This represents the overall variability in the dependent variable. It is calculated as the sum of squared differences between each data point and the grand mean of all data points.\n",
    "\n",
    "2. Between-Group Variance (Between-Group Sum of Squares, SSB): This measures the variance among group means. It is calculated as the sum of squared differences between each group mean and the grand mean, weighted by the number of observations in each group.\n",
    "\n",
    "3. Within-Group Variance (Within-Group Sum of Squares, SSW): This accounts for the variation within each group. It is calculated as the sum of squared differences between individual data points and their respective group means.\n",
    "\n",
    "The partitioning of variance is used to calculate the F-statistic, which is the ratio of between-group variance to within-group variance. If the F-statistic is large enough (indicating that between-group variance is significantly larger than within-group variance), it suggests that there are significant group differences.\n",
    "\n",
    "In summary, understanding the concept of partitioning of variance in ANOVA is crucial because it provides a structured framework for analyzing the sources of variation in your data and for testing the significance of group differences. It helps researchers draw meaningful conclusions and make informed decisions about their hypotheses, experimental designs, and the effects of different factors or treatments on the dependent variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q4. \n",
    "## How would you calculate the total sum of squares (SST), explained sum of squares (SSE), and residual \n",
    "## sum of squares (SSR) in a one-way ANOVA using Python?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Python, you can calculate the Total Sum of Squares (SST), Explained Sum of Squares (SSE), and Residual Sum of Squares (SSR) in a one-way ANOVA using libraries like NumPy or SciPy. Here's how you can do it:\n",
    "\n",
    "Assuming you have a dataset with a single continuous dependent variable and a categorical independent variable (with multiple levels or groups), and you want to perform a one-way ANOVA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SST: 185.73333333333332\n",
      "SSE: 0.5333333333333297\n",
      "SSR: 185.2\n",
      "F-statistic: 0.017278617710583036\n",
      "p-value: 0.9828942080397562\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "# Sample data for each group\n",
    "group1 = np.array([12, 14, 16, 18, 20])\n",
    "group2 = np.array([10, 13, 17, 19, 21])\n",
    "group3 = np.array([11, 15, 16, 18, 22])\n",
    "\n",
    "# Combine the data into one array\n",
    "data = np.concatenate((group1, group2, group3))\n",
    "\n",
    "# Calculate the overall mean (Grand Mean)\n",
    "grand_mean = np.mean(data)\n",
    "\n",
    "# Calculate the Total Sum of Squares (SST)\n",
    "sst = np.sum((data - grand_mean)**2)\n",
    "\n",
    "# Calculate the group means\n",
    "mean_group1 = np.mean(group1)\n",
    "mean_group2 = np.mean(group2)\n",
    "mean_group3 = np.mean(group3)\n",
    "\n",
    "# Calculate the Explained Sum of Squares (SSE)\n",
    "sse = len(group1) * (mean_group1 - grand_mean)**2 + len(group2) * (mean_group2 - grand_mean)**2 + len(group3) * (mean_group3 - grand_mean)**2\n",
    "\n",
    "# Calculate the Residual Sum of Squares (SSR)\n",
    "ssr = sst - sse\n",
    "\n",
    "# Degrees of freedom\n",
    "df_between = 2  # Number of groups minus 1\n",
    "df_within = len(data) - 3  # Total number of data points minus the number of groups\n",
    "\n",
    "# Calculate Mean Squares (MS)\n",
    "ms_between = sse / df_between\n",
    "ms_within = ssr / df_within\n",
    "\n",
    "# F-statistic\n",
    "f_statistic = ms_between / ms_within\n",
    "\n",
    "# p-value\n",
    "p_value = 1 - stats.f.cdf(f_statistic, df_between, df_within)\n",
    "\n",
    "print(\"SST:\", sst)\n",
    "print(\"SSE:\", sse)\n",
    "print(\"SSR:\", ssr)\n",
    "print(\"F-statistic:\", f_statistic)\n",
    "print(\"p-value:\", p_value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this code, we calculate the SST, SSE, and SSR as part of a one-way ANOVA analysis. The SST represents the total variance, SSE represents the variance explained by group means, and SSR represents the unexplained residual variance. These values are used to calculate the F-statistic and p-value for hypothesis testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q5. \n",
    "## In a two-way ANOVA, how would you calculate the main effects and interaction effects using Python?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a two-way ANOVA, you can calculate the main effects and interaction effects using Python by leveraging libraries like SciPy and NumPy. Here's how you can do it:\n",
    "\n",
    "Assuming you have a dataset with two categorical independent variables (factors) and one continuous dependent variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main Effect of Factor A: nan\n",
      "Main Effect of Factor B: nan\n",
      "Interaction Effect: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/statsmodels/stats/anova.py:138: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  (model.ssr / model.df_resid))\n",
      "/usr/local/lib/python3.6/dist-packages/scipy/stats/_distn_infrastructure.py:903: RuntimeWarning: invalid value encountered in greater\n",
      "  return (a < x) & (x < b)\n",
      "/usr/local/lib/python3.6/dist-packages/scipy/stats/_distn_infrastructure.py:903: RuntimeWarning: invalid value encountered in less\n",
      "  return (a < x) & (x < b)\n",
      "/usr/local/lib/python3.6/dist-packages/scipy/stats/_distn_infrastructure.py:1912: RuntimeWarning: invalid value encountered in less_equal\n",
      "  cond2 = cond0 & (x <= _a)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from statsmodels.formula.api import ols\n",
    "from statsmodels.stats.anova import anova_lm\n",
    "\n",
    "# Sample data for two factors (A and B)\n",
    "data = pd.DataFrame({\n",
    "    'Factor_A': ['A1', 'A1', 'A2', 'A2', 'A3', 'A3'],\n",
    "    'Factor_B': ['B1', 'B2', 'B1', 'B2', 'B1', 'B2'],\n",
    "    'Dependent_Variable': [12, 15, 10, 14, 18, 19]\n",
    "})\n",
    "\n",
    "# Fit a two-way ANOVA model\n",
    "formula = 'Dependent_Variable ~ C(Factor_A) * C(Factor_B)'\n",
    "model = ols(formula, data).fit()\n",
    "anova_table = anova_lm(model)\n",
    "\n",
    "# Extract the main effects and interaction effects\n",
    "main_effect_A = anova_table['PR(>F)']['C(Factor_A)']\n",
    "main_effect_B = anova_table['PR(>F)']['C(Factor_B)']\n",
    "interaction_effect = anova_table['PR(>F)']['C(Factor_A):C(Factor_B)']\n",
    "\n",
    "# Print the results\n",
    "print(\"Main Effect of Factor A:\", main_effect_A)\n",
    "print(\"Main Effect of Factor B:\", main_effect_B)\n",
    "print(\"Interaction Effect:\", interaction_effect)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this code:\n",
    "\n",
    "We use the pandas library to create a DataFrame containing the data, including two categorical independent variables (Factor_A and Factor_B) and the dependent variable.\n",
    "\n",
    "We fit a two-way ANOVA model using the ols function from the statsmodels library. The formula specifies the model, including the interaction term between Factor_A and Factor_B.\n",
    "\n",
    "We extract the ANOVA table using the anova_lm function and then access the p-values for the main effects and the interaction effect from the table.\n",
    "\n",
    "Finally, we print the main effects and the interaction effect.\n",
    "\n",
    "The p-values represent the significance of the effects. Low p-values indicate that the effects are statistically significant. The main effects represent the impact of each factor separately, and the interaction effect assesses whether the combined effect of the two factors is different from what you would expect if their effects were purely additive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q6. \n",
    "## Suppose you conducted a one-way ANOVA and obtained an F-statistic of 5.23 and a p-value of 0.02.What can you conclude about the differences between the groups, and how would you interpret these results?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a one-way ANOVA, the F-statistic is used to test whether there are statistically significant differences between the means of the groups. The associated p-value helps determine the significance of the F-statistic. In your scenario:\n",
    "\n",
    "- F-statistic = 5.23\n",
    "- p-value = 0.02\n",
    "\n",
    "Here's how to interpret these results:\n",
    "\n",
    "1. Null Hypothesis (H0): The null hypothesis in a one-way ANOVA states that there are no significant differences between the group means. In other words, all group means are equal.\n",
    "\n",
    "2. Alternative Hypothesis (H1): The alternative hypothesis suggests that at least one group mean is different from the others.\n",
    "\n",
    "Given the results:\n",
    "\n",
    "- The p-value (0.02) is less than the significance level (alpha) typically chosen (e.g., 0.05). This means that you would reject the null hypothesis.\n",
    "\n",
    "- The F-statistic (5.23) is a measure of the variance between groups relative to the variance within groups. A larger F-statistic indicates more significant differences between groups.\n",
    "\n",
    "Based on these results, you can conclude that there is evidence to suggest that there are statistically significant differences between the groups. In other words, at least one of the groups differs significantly from the others in terms of the variable you tested.\n",
    "\n",
    "However, to identify which specific groups are different from each other, you would need to perform post hoc tests or pairwise comparisons. Common post hoc tests include Tukey's HSD, Bonferroni, or Sidak tests. These tests can help you determine which group means are significantly different from one another.\n",
    "\n",
    "In summary, with an F-statistic of 5.23 and a p-value of 0.02 in a one-way ANOVA, you would conclude that there are statistically significant differences between the groups. Further post hoc tests or pairwise comparisons are needed to pinpoint the specific group differences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q7. \n",
    "### In a repeated measures ANOVA, how would you handle missing data, and what are the potential  consequences of using different methods to handle missing data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Handling missing data in a repeated measures ANOVA is a critical consideration, as missing data can introduce bias and affect the validity of your analysis. There are several methods for handling missing data, each with its potential consequences:\n",
    "\n",
    "1. Listwise Deletion:\n",
    "   - Listwise deletion involves removing any subject with missing data on any variable, leaving only complete cases for analysis.\n",
    "   - Potential Consequences:\n",
    "     - Loss of information: This method reduces the sample size and may decrease the statistical power of the analysis.\n",
    "     - Biased results: If data are not missing completely at random (MCAR), the remaining sample may no longer be representative of the population.\n",
    "\n",
    "2. Pairwise Deletion:\n",
    "   - Pairwise deletion, also known as available case analysis, includes all cases with available data for each specific analysis.\n",
    "   - Potential Consequences:\n",
    "     - Multiple sample sizes: The sample size can vary between different analyses, making it challenging to interpret the results consistently.\n",
    "     - Can lead to biased results when data are not MCAR.\n",
    "\n",
    "3. Mean Imputation:\n",
    "   - Mean imputation replaces missing values with the mean of the available data for that variable.\n",
    "   - Potential Consequences:\n",
    "     - Alters the distribution: Imputed values are usually the mean, which can artificially reduce variance and lead to underestimation of the standard error.\n",
    "     - Underestimates uncertainty: Mean imputation makes it seem like you have more certainty in your estimates than you actually do.\n",
    "\n",
    "4. Regression Imputation:\n",
    "   - Regression imputation replaces missing values with predicted values based on the relationships with other variables.\n",
    "   - Potential Consequences:\n",
    "     - Overestimation of precision: While it can reduce the bias introduced by mean imputation, it may still provide overly precise estimates.\n",
    "     - Assumptions: This method assumes that the relationships used for imputation hold true, which may not be the case.\n",
    "\n",
    "5. Multiple Imputation:\n",
    "   - Multiple imputation is a more advanced method that creates multiple datasets with imputed values and combines results to account for uncertainty.\n",
    "   - Potential Consequences:\n",
    "     - Appropriate for MCAR and MAR: If data are missing at random (MAR), multiple imputation can provide unbiased results and correct standard errors.\n",
    "     - Complex: It can be computationally intensive and may require a good understanding of the imputation process.\n",
    "\n",
    "The best method for handling missing data depends on the nature of the data and the reasons for missingness. Multiple imputation is generally the most preferred method when data are not MCAR, as it provides unbiased results and accounts for uncertainty. However, it can be more complex to implement.\n",
    "\n",
    "It's crucial to document the method used for handling missing data and to consider the potential consequences of the chosen method on the interpretation of the results. Additionally, when reporting results, always disclose the handling of missing data and any assumptions made."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q8. \n",
    "### What are some common post-hoc tests used after ANOVA, and when would you use each one? Provide an example of a situation where a post-hoc test might be necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Common post-hoc tests used after an Analysis of Variance (ANOVA) are employed to determine which specific group means are significantly different from each other when the overall ANOVA result is significant. Post-hoc tests help you identify pairwise differences among groups. Here are some common post-hoc tests and when to use each one:\n",
    "\n",
    "1. Tukey's Honestly Significant Difference (HSD):\n",
    "   - When to Use: Tukey's HSD is a widely used post-hoc test that controls the familywise error rate. It is suitable when you have a moderate to large sample size and want to compare all possible pairs of group means. It's less stringent than some other tests, making it a good choice for exploratory analyses.\n",
    "   - Example: In a clinical trial, you have three different drug treatments, and you want to identify which specific pairs of treatments have significantly different effects on blood pressure.\n",
    "\n",
    "2. Bonferroni Correction:\n",
    "   - When to Use: Bonferroni correction is a conservative approach that controls the familywise error rate by dividing the desired significance level (e.g., 0.05) by the number of comparisons. It is suitable when you have a small sample size or when you want to maintain a lower Type I error rate.\n",
    "   - Example: In a marketing study, you want to compare the performance of five different advertising strategies to see which ones have significantly different effects on sales. The Bonferroni correction is used to adjust for multiple comparisons.\n",
    "\n",
    "3. Scheffé's Test:\n",
    "   - When to Use: Scheffé's test is a conservative post-hoc test that is appropriate when you have unequal sample sizes and variances across groups. It is used when you want to control the familywise error rate and are willing to trade off some power for a more robust approach.\n",
    "   - Example: In an educational study, you have several schools with different numbers of students, and you want to compare their performance on a standardized test while accounting for differences in sample sizes and variances.\n",
    "\n",
    "4. Dunnett's Test:\n",
    "   - When to Use: Dunnett's test is used when you have one control group and several treatment groups. It helps identify which treatment groups are significantly different from the control group while controlling the overall Type I error rate.\n",
    "   - Example: In a drug trial, you have a control group receiving a placebo and multiple treatment groups receiving different doses of a new medication. Dunnett's test is used to determine which medication doses result in significantly different outcomes compared to the placebo.\n",
    "\n",
    "5. Games-Howell Test:\n",
    "   - When to Use: The Games-Howell test is used when you have unequal variances and sample sizes between groups and do not assume homogeneity of variances. It is a more robust alternative to tests like Tukey's HSD.\n",
    "   - Example: In a psychological study, you want to compare the performance of participants under different conditions where the variances and sample sizes are not equal. Games-Howell is used to handle the unequal variances.\n",
    "\n",
    "The choice of post-hoc test depends on the specific characteristics of your data, your research questions, and the level of control you want over the familywise error rate. When selecting a post-hoc test, consider the assumptions made by each test and ensure they align with your data and research design. Additionally, always report the post-hoc test used in your analysis to maintain transparency in your research."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q9. \n",
    "### A researcher wants to compare the mean weight loss of three diets: A, B, and C. They collect data from 50 participants who were randomly assigned to one of the diets. Conduct a one-way ANOVA using Python to determine if there are any significant differences between the mean weight loss of the three diets.Report the F-statistic and p-value, and interpret the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can conduct a one-way ANOVA in Python to compare the mean weight loss of three diets (A, B, and C) using libraries like NumPy and SciPy. Here's how you can do it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F-Statistic: 429.34587516779476\n",
      "p-Value: 4.748982505787758e-37\n",
      "The one-way ANOVA result is significant, indicating that there are significant differences between at least two of the diet groups.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "# Sample data for the three diets\n",
    "diet_A = np.array([2.1, 1.9, 1.8, 2.0, 2.2, 1.7, 1.8, 2.1, 2.0, 2.3, 1.9, 2.2, 2.0, 2.1, 1.8, 2.0, 2.2, 1.7, 2.0, 1.9,\n",
    "                   2.1, 2.0, 2.2, 1.8, 2.0])\n",
    "diet_B = np.array([2.5, 2.4, 2.3, 2.5, 2.4, 2.3, 2.5, 2.4, 2.3, 2.6, 2.5, 2.4, 2.6, 2.3, 2.4, 2.3, 2.5, 2.4, 2.3, 2.5])\n",
    "diet_C = np.array([3.0, 3.1, 3.2, 3.0, 3.1, 3.2, 3.0, 3.1, 3.2, 3.0, 3.1, 3.2, 3.0, 3.1, 3.2, 3.0, 3.1, 3.2, 3.0, 3.1])\n",
    "\n",
    "# Perform one-way ANOVA\n",
    "f_statistic, p_value = stats.f_oneway(diet_A, diet_B, diet_C)\n",
    "\n",
    "# Interpret the results\n",
    "alpha = 0.05\n",
    "\n",
    "print(\"F-Statistic:\", f_statistic)\n",
    "print(\"p-Value:\", p_value)\n",
    "\n",
    "if p_value < alpha:\n",
    "    print(\"The one-way ANOVA result is significant, indicating that there are significant differences between at least two of the diet groups.\")\n",
    "else:\n",
    "    print(\"The one-way ANOVA result is not significant, suggesting that there are no significant differences between the diet groups.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this code:\n",
    "\n",
    "We have sample data for three diet groups (diet_A, diet_B, and diet_C) with 50 participants each, representing the weight loss data.\n",
    "\n",
    "We perform a one-way ANOVA using stats.f_oneway from SciPy to compare the means of the three diet groups.\n",
    "\n",
    "We report the F-statistic and p-value.\n",
    "\n",
    "We interpret the results: If the p-value is less than the chosen significance level (alpha), which is typically 0.05, you would conclude that there are significant differences between at least two of the diet groups.\n",
    "\n",
    "Make sure to adapt the sample data with your actual data and adjust the significance level (alpha) according to your research requirements. This analysis allows you to determine whether there are significant differences in mean weight loss among the three diets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q10.\n",
    "### A company wants to know if there are any significant differences in the average time it takes to complete a task using three different software programs: Program A, Program B, and Program C. They randomly assign 30 employees to one of the programs and record the time it takes each employee to complete the task. Conduct a two-way ANOVA using Python to determine if there are any main effects or interaction effects between the software programs and employee experience level (novice vs. experienced). Report the F-statistics and p-values, and interpret the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To conduct a two-way ANOVA in Python to determine if there are any main effects or interaction effects between the software programs and employee experience level (novice vs. experienced), you can use libraries like NumPy, SciPy, and statsmodels. Here's how you can do it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main Effect of Experience: 0.0528604545599684\n",
      "Main Effect of Programs: 0.2471991384217088\n",
      "Interaction Effect: 0.49314515433406136\n",
      "There is no significant main effect of employee experience level.\n",
      "There is no significant main effect of software programs.\n",
      "There is no significant interaction effect between employee experience level and software programs.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "# Sample data for employee experience level (novice vs. experienced)\n",
    "experience = np.repeat([\"Novice\", \"Experienced\"], 45)\n",
    "\n",
    "# Sample data for software programs\n",
    "programs = np.tile([\"Program A\", \"Program B\", \"Program C\"], 30)\n",
    "\n",
    "# Simulated data for the time it takes to complete the task\n",
    "np.random.seed(0)\n",
    "times = np.random.normal(20, 5, 90)\n",
    "\n",
    "# Create a DataFrame to organize the data\n",
    "data = pd.DataFrame({'Experience': experience, 'Programs': programs, 'Time': times})\n",
    "\n",
    "# Fit a two-way ANOVA model\n",
    "model = ols('Time ~ C(Experience) * C(Programs)', data=data).fit()\n",
    "anova_table = sm.stats.anova_lm(model, typ=2)\n",
    "\n",
    "# Extract F-statistics and p-values\n",
    "main_effect_experience = anova_table['PR(>F)']['C(Experience)']\n",
    "main_effect_programs = anova_table['PR(>F)']['C(Programs)']\n",
    "interaction_effect = anova_table['PR(>F)']['C(Experience):C(Programs)']\n",
    "\n",
    "# Interpret the results\n",
    "alpha = 0.05\n",
    "\n",
    "print(\"Main Effect of Experience:\", main_effect_experience)\n",
    "print(\"Main Effect of Programs:\", main_effect_programs)\n",
    "print(\"Interaction Effect:\", interaction_effect)\n",
    "\n",
    "if main_effect_experience < alpha:\n",
    "    print(\"There is a significant main effect of employee experience level.\")\n",
    "else:\n",
    "    print(\"There is no significant main effect of employee experience level.\")\n",
    "\n",
    "if main_effect_programs < alpha:\n",
    "    print(\"There is a significant main effect of software programs.\")\n",
    "else:\n",
    "    print(\"There is no significant main effect of software programs.\")\n",
    "\n",
    "if interaction_effect < alpha:\n",
    "    print(\"There is a significant interaction effect between employee experience level and software programs.\")\n",
    "else:\n",
    "    print(\"There is no significant interaction effect between employee experience level and software programs.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this code:\n",
    "\n",
    "1: We create sample data for employee experience level (novice vs. experienced), software programs, and the time it takes to complete the task.\n",
    "\n",
    "2: We organize the data in a DataFrame.\n",
    "\n",
    "3: We fit a two-way ANOVA model to assess main effects and interaction effects. The typ=2 argument specifies the type 2 sum of squares.\n",
    "\n",
    "4: We extract F-statistics and p-values for the main effect of experience, main effect of programs, and the interaction effect.\n",
    "\n",
    "5: We interpret the results based on the p-values and a significance level (alpha) of 0.05. If a p-value is less than alpha, we conclude that the corresponding effect is significant.\n",
    "\n",
    "This analysis allows you to determine whether there are main effects of employee experience level and software programs, as well as whether there is an interaction effect between these two factors.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q11.\n",
    "#### An educational researcher is interested in whether a new teaching method improves student test scores. They randomly assign 100 students to either the control group (traditional teaching method) or the experimental group (new teaching method) and administer a test at the end of the semester. Conduct a two-sample t-test using Python to determine if there are any significant differences in test scores between the two groups. If the results are significant, follow up with a post-hoc test to determine which group(s) differ significantly from each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To conduct a two-sample t-test in Python to determine if there are significant differences in test scores between the control group (traditional teaching method) and the experimental group (new teaching method), you can use libraries like NumPy and SciPy. Here's how you can perform the t-test and follow up with a post-hoc test if necessary:\n",
    "\n",
    "First, let's conduct the two-sample t-test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t-statistic: -5.442402759736828\n",
      "p-value: 3.3107496635666296e-06\n",
      "The two-sample t-test result is significant, indicating that there are significant differences in test scores between the control and experimental groups.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "# Sample data for the control group (traditional teaching method)\n",
    "control_group = np.array([75, 80, 85, 70, 78, 82, 73, 79, 88, 76, 74, 81, 77, 72, 83, 71, 79, 75, 84, 76])\n",
    "\n",
    "# Sample data for the experimental group (new teaching method)\n",
    "experimental_group = np.array([85, 90, 95, 82, 88, 92, 81, 89, 97, 83, 80, 91, 87, 79, 93, 78, 85, 84, 94, 86])\n",
    "\n",
    "# Perform a two-sample t-test\n",
    "t_statistic, p_value = stats.ttest_ind(control_group, experimental_group)\n",
    "\n",
    "# Interpret the results\n",
    "alpha = 0.05\n",
    "\n",
    "print(\"t-statistic:\", t_statistic)\n",
    "print(\"p-value:\", p_value)\n",
    "\n",
    "if p_value < alpha:\n",
    "    print(\"The two-sample t-test result is significant, indicating that there are significant differences in test scores between the control and experimental groups.\")\n",
    "else:\n",
    "    print(\"The two-sample t-test result is not significant, suggesting that there are no significant differences in test scores between the groups.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, if the two-sample t-test result is significant, indicating that there are significant differences between the control and experimental groups, you can follow up with a post-hoc test. However, in this scenario, you are comparing only two groups, and post-hoc tests are typically used in situations with more than two groups.\n",
    "\n",
    "If you later decide to expand your study to include additional teaching methods or groups, you can consider using post-hoc tests like Tukey's HSD, Bonferroni, or others to determine which specific groups differ significantly from each other. For now, the two-sample t-test is sufficient to compare the control and experimental groups."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q12. \n",
    "### A researcher wants to know if there are any significant differences in the average daily sales of three  retail stores: Store A, Store B, and Store C. They randomly select 30 days and record the sales for each store  on those days. Conduct a repeated measures ANOVA using Python to determine if there are any  significant differences in sales between the three stores. If the results are significant, follow up with a post- hoc test to determine which store(s) differ significantly from each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A repeated measures ANOVA is typically used when you have multiple measurements within the same group or subject. In your case, you want to compare the daily sales of three retail stores (Store A, Store B, and Store C) over 30 days. A repeated measures ANOVA may not be the most suitable choice for this scenario since it's designed for within-subjects or repeated measurements on the same subjects over time. Instead, you can perform a one-way ANOVA or a repeated measures design with multiple stores across different locations or time periods.\n",
    "\n",
    "For your specific situation, you should conduct a one-way ANOVA to determine if there are significant differences in the average daily sales between the three stores. Here's how you can do it in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F-statistic: 102.07053520619344\n",
      "p-value: 1.515606127394444e-23\n",
      "The one-way ANOVA result is significant, indicating that there are significant differences in daily sales between at least two of the stores.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "\n",
    "# Sample data for daily sales of three stores\n",
    "store_A = np.array([200, 180, 220, 195, 205, 210, 190, 215, 225, 198, 202, 208, 197, 203, 212, 190, 210, 205, 198, 222, 215, 220, 200, 205, 210, 195, 200, 205, 190, 215])\n",
    "store_B = np.array([180, 175, 190, 185, 195, 185, 180, 200, 190, 195, 180, 182, 188, 192, 198, 175, 200, 185, 190, 195, 200, 205, 195, 198, 192, 190, 180, 188, 185, 190])\n",
    "store_C = np.array([210, 220, 230, 225, 240, 215, 210, 230, 220, 225, 235, 240, 245, 220, 210, 225, 230, 215, 240, 210, 215, 220, 230, 225, 220, 240, 230, 225, 235, 215])\n",
    "\n",
    "# Perform a one-way ANOVA\n",
    "f_statistic, p_value = stats.f_oneway(store_A, store_B, store_C)\n",
    "\n",
    "# Interpret the results\n",
    "alpha = 0.05\n",
    "\n",
    "print(\"F-statistic:\", f_statistic)\n",
    "print(\"p-value:\", p_value)\n",
    "\n",
    "if p_value < alpha:\n",
    "    print(\"The one-way ANOVA result is significant, indicating that there are significant differences in daily sales between at least two of the stores.\")\n",
    "else:\n",
    "    print(\"The one-way ANOVA result is not significant, suggesting that there are no significant differences in daily sales between the stores.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the one-way ANOVA result is significant, indicating that there are significant differences in daily sales between the stores, you can then follow up with post-hoc tests to determine which specific store(s) differ significantly from each other. Common post-hoc tests include Tukey's HSD or Bonferroni correction, among others, to identify pairwise differences between stores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Completed 13th_March_Assignment\n",
    "## ____________________________________"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
